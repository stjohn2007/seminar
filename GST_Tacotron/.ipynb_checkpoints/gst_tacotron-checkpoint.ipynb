{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cc8c1dca-e73d-4b08-8a7b-068eee69941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9e734-661c-458f-acd1-8d48bb70f13d",
   "metadata": {},
   "source": [
    "## ReferenceEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de0aa683-3bda-43ee-8bd9-ac1e812786cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferenceEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mel = 80,\n",
    "        conv_channels1 = 32,\n",
    "        conv_channels2 = 32,\n",
    "        conv_channels3 = 64,\n",
    "        conv_channels4 = 64,\n",
    "        conv_channels5 = 128,\n",
    "        conv_channels6 = 128,\n",
    "        n_unit = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        conv_channels_list = [\n",
    "            conv_channels1,\n",
    "            conv_channels2,\n",
    "            conv_channels3,\n",
    "            conv_channels4,\n",
    "            conv_channels5,\n",
    "            conv_channels6,\n",
    "        ]\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        for layer in range(6):\n",
    "            n_mel = (n_mel + 1) // 2\n",
    "            \n",
    "            in_channels = 1 if layer == 0 else conv_channels_list[layer-1]\n",
    "            out_channels = conv_channels_list[layer]\n",
    "            self.convs += [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        self.convs = nn.Sequential(*self.convs)\n",
    "        \n",
    "        n_mel *= conv_channels_list[-1]\n",
    "        self.gru = nn.GRU(n_mel, n_unit, batch_first=True)\n",
    "        \n",
    "    def forward(self, seqs):\n",
    "        out = self.convs(seqs) # (B, 1, seq_len, n_mel) -> (B, channels, new_seq_len, new_n_mel)\n",
    "        out = torch.squeeze(out.reshape(out.shape[0], 1, out.shape[2], -1)) # (B, channels, new_seq_len, new_n_mel) -> (B, new_seq_len, channels * new_n_mel)\n",
    "        out = self.gru(out) # (B, new_seq_len, channels * new_n_mel) -> (B, new_seq_len, n_unit)\n",
    "        return out[0], out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fcb1be25-89f0-406e-85f8-1386f96f1529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 250, 128])\n"
     ]
    }
   ],
   "source": [
    "ref = ReferenceEncoder()\n",
    "ref.cuda()\n",
    "input = torch.zeros(10, 1, 16000, 80).to(\"cuda\")\n",
    "ref_output, hidden_state = ref(input)\n",
    "print(ref_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9bff91-3d03-42af-9eb5-cd2d5ee7a798",
   "metadata": {},
   "source": [
    "## StyleTokenLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3f46e2f3-f18d-4b3c-8a08-f9744f2da06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qiita.com/m__k/items/646044788c5f94eadc8d\n",
    "\n",
    "class StyleTokenLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_size = 256,\n",
    "        n_tokens = 10,\n",
    "        device = \"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.emb_size = emb_size,\n",
    "        self.softmax = nn.Softmax(dim=2) # n_token方向にsoftmaxを取りたい\n",
    "        self.tokens = torch.randn(n_tokens, emb_size).to(device)\n",
    "    \n",
    "    def forward(self, query=None, token_num=0):\n",
    "        if query == None: #inference\n",
    "            return self.tokens[token_num]\n",
    "        \n",
    "        query = torch.cat((query, query), 2) # この実装があってるかどうかわからない, ReferenceEncoderの出力次元が128でtokenのembeddingの次元が256なのでqueryを2つ重ねている\n",
    "        tokens = self.tanh(self.tokens)\n",
    "        key = tokens.repeat(query.shape[0], 1, 1) # (n_token, emb_size) -> (B, n_token, emb_size)\n",
    "        value = key\n",
    "        key = torch.transpose(key, 1, 2) # (B, n_token, emb_size) -> (B, emb_size, n_token)\n",
    "        s = torch.bmm(query, key) / self.emb_size[0]**(1/2) # (B, seq_len, emb_size) @ (B, emb_size, n_token) -> (B, seq_len, n_token)  論文ではcosine similarityを用いていたが実装の簡単のためdot-productにした\n",
    "        attention_weight = self.softmax(s) # (B, seq_len, n_token) -> (B, seq_len, n_token)\n",
    "        out = torch.bmm(attention_weight, value) #(B, seq_len, n_token) @ (B, n_token, emb_size) -> (B, seq_len, emb_size)\n",
    "        out = torch.mean(out, 1) # (B, seq_len, emb_size) -> (B, emb_size)\n",
    "        \n",
    "        return out, attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4fbe7f05-623b-431d-876c-fa3137802e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "style = StyleTokenLayer()\n",
    "style.cuda()\n",
    "style_output, attention_weight = style(ref_output)\n",
    "print(style_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edab0da-1f6c-4407-9200-be0cdfaf9b3d",
   "metadata": {},
   "source": [
    "### GSTLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b1e820ed-ed70-4f06-a8ab-182fa1bcf40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class GSTLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mel=80,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ref = ReferenceEncoder(n_mel=n_mel).cuda()\n",
    "        self.style = StyleTokenLayer().cuda()\n",
    "        \n",
    "    def forward(self, seqs=None, token_num=0): #inference時の挙動はreference audioを持ってくる形ではなく、tokenを指定してそれをembeddingとして用いるようにした\n",
    "        if seqs == None: #inference\n",
    "            token = self.style(token_num=token_num)\n",
    "            return token\n",
    "        else:\n",
    "            seqs = pad_sequence(seqs, batch_first=True)\n",
    "            seqs = torch.unsqueeze(seqs, 1)\n",
    "            ref_out, ref_hidden = self.ref(seqs)\n",
    "            style_out, style_weight = self.style(ref_out)\n",
    "            style_out = torch.unsqueeze(style_out,1)\n",
    "            return style_out, style_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7500cfb9-9d8f-4ac5-9192-4d27d542e7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "gstLayer = GSTLayer()\n",
    "gstLayer.cuda()\n",
    "input = [torch.zeros(15000, 80).to(\"cuda\"), torch.zeros(16000, 80).to(\"cuda\")]\n",
    "out, att = gstLayer(input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8881120-daf1-47ee-9c76-25f1a601572c",
   "metadata": {},
   "source": [
    "あとやること\n",
    "StyleTokenLayerの出力expand asでexpandして、encoderの出力に足してdecoderに突っ込むようにいじる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561d419-1894-4caf-8fd6-22e280b347c2",
   "metadata": {},
   "source": [
    "## TextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "30eba7b1-41c9-4587-b9fc-11d24b767bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_vocab=40,\n",
    "        embed_dim=256,\n",
    "        conv_layers=3,\n",
    "        conv_channels=256,\n",
    "        conv_kernel_size=5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 文字埋め込み\n",
    "        self.embed = nn.Embedding(num_vocab, embed_dim, padding_idx=0)\n",
    "\n",
    "        # 1次元畳み込みの重ね合わせ：局所的な依存関係のモデル化\n",
    "        self.convs = nn.ModuleList()\n",
    "        for layer in range(conv_layers):\n",
    "            in_channels = embed_dim if layer == 0 else conv_channels\n",
    "            self.convs += [\n",
    "                nn.Conv1d(\n",
    "                    in_channels,\n",
    "                    conv_channels,\n",
    "                    conv_kernel_size,\n",
    "                    padding=(conv_kernel_size - 1) // 2,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(conv_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "            ]\n",
    "        self.convs = nn.Sequential(*self.convs)\n",
    "\n",
    "    def forward(self, seqs):\n",
    "        emb = self.embed(seqs)\n",
    "        # 1 次元畳み込みと embedding では、入力のサイズが異なるので注意\n",
    "        out = self.convs(emb.transpose(1, 2)).transpose(1, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3c68448-7434-4e91-9492-be6f90247ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvEncoder(\n",
       "  (embed): Embedding(40, 256, padding_idx=0)\n",
       "  (convs): Sequential(\n",
       "    (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c0f14722-1b62-4454-bdf7-1f5699970b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(ConvEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_vocab=40,\n",
    "        embed_dim=512,\n",
    "        hidden_dim=256,\n",
    "        conv_layers=3,\n",
    "        conv_channels=512,\n",
    "        conv_kernel_size=5,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            num_vocab, embed_dim, conv_layers, conv_channels, conv_kernel_size\n",
    "        )\n",
    "        # 双方向 LSTM による長期依存関係のモデル化\n",
    "        self.blstm = nn.LSTM(\n",
    "            conv_channels, hidden_dim // 2, 1, batch_first=True, bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, seqs, in_lens):\n",
    "        emb = self.embed(seqs)\n",
    "        # 1 次元畳み込みと embedding では、入力のサイズ が異なるので注意\n",
    "        out = self.convs(emb.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        # 双方向 LSTM の計算\n",
    "        out = pack_padded_sequence(out, in_lens, batch_first=True)\n",
    "        out, _ = self.blstm(out)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d635b7b0-611f-4fe6-bc7f-b444a2f81051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embed): Embedding(40, 512, padding_idx=0)\n",
       "  (convs): Sequential(\n",
       "    (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (blstm): LSTM(512, 128, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828183d4-0663-4021-919b-778001aa5992",
   "metadata": {},
   "source": [
    "## LocationSensitivaAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ce647ba-9bfa-4aff-b363-b85e1a1d327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationSensitiveAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_dim=512,\n",
    "        decoder_dim=1024,\n",
    "        hidden_dim=128,\n",
    "        conv_channels=32,\n",
    "        conv_kernel_size=31,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.V = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.W = nn.Linear(decoder_dim, hidden_dim, bias=False)\n",
    "        self.U = nn.Linear(conv_channels, hidden_dim, bias=False)\n",
    "        self.F = nn.Conv1d(\n",
    "            1,\n",
    "            conv_channels,\n",
    "            conv_kernel_size,\n",
    "            padding=(conv_kernel_size - 1) // 2,\n",
    "            bias=False,\n",
    "        )\n",
    "        # NOTE: 本書の数式通りに実装するなら bias=False ですが、実用上は bias=True としても問題ありません\n",
    "        self.w = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outs, src_lens, decoder_state, att_prev, mask=None):\n",
    "        # アテンション重みを一様分布で初期化\n",
    "        if att_prev is None:\n",
    "            att_prev = 1.0 - make_pad_mask(src_lens).to(\n",
    "                device=decoder_state.device, dtype=decoder_state.dtype\n",
    "            )\n",
    "            att_prev = att_prev / src_lens.unsqueeze(-1).to(encoder_outs.device)\n",
    "\n",
    "        # (B x T_enc) -> (B x 1 x T_enc) -> (B x conv_channels x T_enc) ->\n",
    "        # (B x T_enc x conv_channels)\n",
    "        f = self.F(att_prev.unsqueeze(1)).transpose(1, 2)\n",
    "\n",
    "        # 式 (9.13) の計算\n",
    "        erg = self.w(\n",
    "            torch.tanh(\n",
    "                self.W(decoder_state).unsqueeze(1) + self.V(encoder_outs) + self.U(f)\n",
    "            )\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            erg.masked_fill_(mask, -float(\"inf\"))\n",
    "\n",
    "        attention_weights = F.softmax(erg, dim=1)\n",
    "\n",
    "        # エンコーダ出力の長さ方向に対して重み付き和を取ります\n",
    "        attention_context = torch.sum(\n",
    "            encoder_outs * attention_weights.unsqueeze(-1), dim=1\n",
    "        )\n",
    "\n",
    "        return attention_context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e612ca5-71d9-4270-b940-04c5b249dd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocationSensitiveAttention(\n",
       "  (V): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (W): Linear(in_features=1024, out_features=128, bias=False)\n",
       "  (U): Linear(in_features=32, out_features=128, bias=False)\n",
       "  (F): Conv1d(1, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
       "  (w): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LocationSensitiveAttention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abdd43b-bf6b-4d43-a1f3-72abbe3decfc",
   "metadata": {},
   "source": [
    "## Pre-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e644b2ae-db99-4c53-823b-80032b90b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, layers=2, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        prenet = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            prenet += [\n",
    "                nn.Linear(in_dim if layer == 0 else hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        self.prenet = nn.Sequential(*prenet)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.prenet:\n",
    "            # 学習時、推論時の両方で Dropout を適用します\n",
    "            x = F.dropout(layer(x), self.dropout, training=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ef802fc-67fa-42f6-a5a8-cb30ffd8ef52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prenet(\n",
       "  (prenet): Sequential(\n",
       "    (0): Linear(in_features=80, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prenet(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff302133-3401-46a9-9728-875010e82844",
   "metadata": {},
   "source": [
    "## ZoneOutCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f960432c-63e9-474f-8a86-a3a038dd67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZoneOutCell(nn.Module):\n",
    "    def __init__(self, cell, zoneout=0.1):\n",
    "        super().__init__()\n",
    "        self.cell = cell\n",
    "        self.hidden_size = cell.hidden_size\n",
    "        self.zoneout = zoneout\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        next_hidden = self.cell(inputs, hidden)\n",
    "        next_hidden = self._zoneout(hidden, next_hidden, self.zoneout)\n",
    "        return next_hidden\n",
    "\n",
    "    def _zoneout(self, h, next_h, prob):\n",
    "        h_0, c_0 = h\n",
    "        h_1, c_1 = next_h\n",
    "        h_1 = self._apply_zoneout(h_0, h_1, prob)\n",
    "        c_1 = self._apply_zoneout(c_0, c_1, prob)\n",
    "        return h_1, c_1\n",
    "\n",
    "    def _apply_zoneout(self, h, next_h, prob):\n",
    "        if self.training:\n",
    "            mask = h.new(*h.size()).bernoulli_(prob)\n",
    "            return mask * h + (1 - mask) * next_h\n",
    "        else:\n",
    "            return prob * h + (1 - prob) * next_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69154dd-4fef-425b-8e6a-35cfe6c40966",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81323548-dd89-4495-9a3f-a2541e81aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_hidden_dim=512,\n",
    "        out_dim=80,\n",
    "        layers=2,\n",
    "        hidden_dim=1024,\n",
    "        prenet_layers=2,\n",
    "        prenet_hidden_dim=256,\n",
    "        prenet_dropout=0.5,\n",
    "        zoneout=0.1,\n",
    "        reduction_factor=1,\n",
    "        attention_hidden_dim=128,\n",
    "        attention_conv_channels=32,\n",
    "        attention_conv_kernel_size=31,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        # 注意機構\n",
    "        self.attention = LocationSensitiveAttention(\n",
    "            encoder_hidden_dim,\n",
    "            hidden_dim,\n",
    "            attention_hidden_dim,\n",
    "            attention_conv_channels,\n",
    "            attention_conv_kernel_size,\n",
    "        )\n",
    "        self.reduction_factor = reduction_factor\n",
    "\n",
    "        # Prenet\n",
    "        self.prenet = Prenet(out_dim, prenet_layers, prenet_hidden_dim, prenet_dropout)\n",
    "\n",
    "        # 片方向LSTM\n",
    "        self.lstm = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            lstm = nn.LSTMCell(\n",
    "                encoder_hidden_dim + prenet_hidden_dim if layer == 0 else hidden_dim,\n",
    "                hidden_dim,\n",
    "            )\n",
    "            lstm = ZoneOutCell(lstm, zoneout)\n",
    "            self.lstm += [lstm]\n",
    "\n",
    "        # 出力への projection 層\n",
    "        proj_in_dim = encoder_hidden_dim + hidden_dim\n",
    "        self.feat_out = nn.Linear(proj_in_dim, out_dim * reduction_factor, bias=False)\n",
    "        self.prob_out = nn.Linear(proj_in_dim, reduction_factor)\n",
    "\n",
    "    def _zero_state(self, hs):\n",
    "        init_hs = hs.new_zeros(hs.size(0), self.lstm[0].hidden_size)\n",
    "        return init_hs\n",
    "\n",
    "    def forward(self, encoder_outs, in_lens, decoder_targets=None):\n",
    "        is_inference = decoder_targets is None\n",
    "\n",
    "        # Reduction factor に基づくフレーム数の調整\n",
    "        # (B, Lmax, out_dim) ->  (B, Lmax/r, out_dim)\n",
    "        if self.reduction_factor > 1 and not is_inference:\n",
    "            decoder_targets = decoder_targets[\n",
    "                :, self.reduction_factor - 1 :: self.reduction_factor\n",
    "            ]\n",
    "\n",
    "        # デコーダの系列長を保持\n",
    "        # 推論時は、エンコーダの系列長から経験的に上限を定める\n",
    "        if is_inference:\n",
    "            max_decoder_time_steps = int(encoder_outs.shape[1] * 10.0)\n",
    "        else:\n",
    "            max_decoder_time_steps = decoder_targets.shape[1]\n",
    "\n",
    "        # ゼロパディングされた部分に対するマスク\n",
    "        mask = make_pad_mask(in_lens).to(encoder_outs.device)\n",
    "\n",
    "        # LSTM の状態をゼロで初期化\n",
    "        h_list, c_list = [], []\n",
    "        for _ in range(len(self.lstm)):\n",
    "            h_list.append(self._zero_state(encoder_outs))\n",
    "            c_list.append(self._zero_state(encoder_outs))\n",
    "\n",
    "        # デコーダの最初の入力\n",
    "        go_frame = encoder_outs.new_zeros(encoder_outs.size(0), self.out_dim)\n",
    "        prev_out = go_frame\n",
    "\n",
    "        # 1つ前の時刻のアテンション重み\n",
    "        prev_att_w = None\n",
    "\n",
    "        # メインループ\n",
    "        outs, logits, att_ws = [], [], []\n",
    "        t = 0\n",
    "        while True:\n",
    "            # コンテキストベクトル、アテンション重みの計算\n",
    "            att_c, att_w = self.attention(\n",
    "                encoder_outs, in_lens, h_list[0], prev_att_w, mask\n",
    "            )\n",
    "\n",
    "            # Pre-Net\n",
    "            prenet_out = self.prenet(prev_out)\n",
    "\n",
    "            # LSTM\n",
    "            xs = torch.cat([att_c, prenet_out], dim=1)\n",
    "            h_list[0], c_list[0] = self.lstm[0](xs, (h_list[0], c_list[0]))\n",
    "            for i in range(1, len(self.lstm)):\n",
    "                h_list[i], c_list[i] = self.lstm[i](\n",
    "                    h_list[i - 1], (h_list[i], c_list[i])\n",
    "                )\n",
    "            # 出力の計算\n",
    "            hcs = torch.cat([h_list[-1], att_c], dim=1)\n",
    "            outs.append(self.feat_out(hcs).view(encoder_outs.size(0), self.out_dim, -1))\n",
    "            logits.append(self.prob_out(hcs))\n",
    "            att_ws.append(att_w)\n",
    "\n",
    "            # 次の時刻のデコーダの入力を更新\n",
    "            if is_inference:\n",
    "                prev_out = outs[-1][:, :, -1]  # (1, out_dim)\n",
    "            else:\n",
    "                # Teacher forcing\n",
    "                prev_out = decoder_targets[:, t, :]\n",
    "\n",
    "            # 累積アテンション重み\n",
    "            prev_att_w = att_w if prev_att_w is None else prev_att_w + att_w\n",
    "\n",
    "            t += 1\n",
    "            # 停止条件のチェック\n",
    "            if t >= max_decoder_time_steps:\n",
    "                break\n",
    "            if is_inference and (torch.sigmoid(logits[-1]) >= 0.5).any():\n",
    "                break\n",
    "                \n",
    "        # 各時刻の出力を結合\n",
    "        logits = torch.cat(logits, dim=1)  # (B, Lmax)\n",
    "        outs = torch.cat(outs, dim=2)  # (B, out_dim, Lmax)\n",
    "        att_ws = torch.stack(att_ws, dim=1)  # (B, Lmax, Tmax)\n",
    "\n",
    "        if self.reduction_factor > 1:\n",
    "            outs = outs.view(outs.size(0), self.out_dim, -1)  # (B, out_dim, Lmax)\n",
    "\n",
    "        return outs, logits, att_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d74cec-6894-4358-a3c3-b19b8aabcc48",
   "metadata": {},
   "source": [
    "## Post-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dd429745-a8f9-44f7-9fdc-3f045bb7a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Postnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim=80,\n",
    "        layers=5,\n",
    "        channels=512,\n",
    "        kernel_size=5,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        postnet = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            in_channels = in_dim if layer == 0 else channels\n",
    "            out_channels = in_dim if layer == layers - 1 else channels\n",
    "            postnet += [\n",
    "                nn.Conv1d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=(kernel_size - 1) // 2,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            ]\n",
    "            if layer != layers - 1:\n",
    "                postnet += [nn.Tanh()]\n",
    "            postnet += [nn.Dropout(dropout)]\n",
    "        self.postnet = nn.Sequential(*postnet)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return self.postnet(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2edbbc6e-41cb-438d-b116-6f6108d06acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Postnet(\n",
       "  (postnet): Sequential(\n",
       "    (0): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Tanh()\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Tanh()\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "    (12): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): Tanh()\n",
       "    (15): Dropout(p=0.5, inplace=False)\n",
       "    (16): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (17): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Postnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4f41b-4c4f-4328-bc76-00227f0af38d",
   "metadata": {},
   "source": [
    "あとはつなげる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd935f-7e0f-480e-8644-9d920e133bf0",
   "metadata": {},
   "source": [
    "### GST-Tacotronモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "296e1fad-4766-447e-b464-7bf73ea7d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GST_Tacotron(nn.Module):\n",
    "    def __init__(self\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.gst = GSTLayer()\n",
    "        self.decoder = Decoder()\n",
    "        self.postnet = Postnet()\n",
    "\n",
    "    def forward(self, seq, in_lens, decoder_targets):\n",
    "        seq = seq.to(\"cuda\")\n",
    "        # エンコーダによるテキストに潜在する表現の獲得\n",
    "        encoder_outs = self.encoder(seq, in_lens)\n",
    "        \n",
    "        # GSTによる音声に潜在する表現の獲得\n",
    "        gst_outs, gst_att_ws = self.gst(decoder_targets)\n",
    "        \n",
    "        # エンコーダの出力とGSTの出力を足す\n",
    "        encoder_outs += gst_outs.expand_as(encoder_outs) \n",
    "        encoder_outs = encoder_outs.repeat(1, 1, 2)\n",
    "        print(encoder_outs.size())\n",
    "\n",
    "        # デコーダによるメルスペクトログラム、stop token の予測\n",
    "        outs, logits, att_ws = self.decoder(encoder_outs, in_lens, torch.stack(decoder_targets))\n",
    "\n",
    "        # Post-Net によるメルスペクトログラムの残差の予測\n",
    "        outs_fine = outs + self.postnet(outs)\n",
    "\n",
    "        # (B, C, T) -> (B, T, C)\n",
    "        outs = outs.transpose(2, 1)\n",
    "        outs_fine = outs_fine.transpose(2, 1)\n",
    "\n",
    "        return outs, outs_fine, encoder_outs, logits, att_ws, gst_att_ws \n",
    "    \n",
    "    def inference(self, seq):\n",
    "        seq = seq.unsqueeze(0) if len(seq.shape) == 1 else seq\n",
    "        in_lens = torch.tensor([seq.shape[-1]], dtype=torch.long, device=seq.device)\n",
    "\n",
    "        return self.forward(seq, in_lens, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6186ca5-33d0-443a-8d94-268f7bec67f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a8eedfd4-8cae-40df-ae83-08794414765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_1d(x, max_len, constant_values=0):\n",
    "    \"\"\"Pad a 1d-tensor.\n",
    "    Args:\n",
    "        x (torch.Tensor): tensor to pad\n",
    "        max_len (int): maximum length of the tensor\n",
    "        constant_values (int, optional): value to pad with. Default: 0\n",
    "    Returns:\n",
    "        torch.Tensor: padded tensor\n",
    "    \"\"\"\n",
    "    x = np.pad(\n",
    "        x,\n",
    "        (0, max_len - len(x)),\n",
    "        mode=\"constant\",\n",
    "        constant_values=constant_values,\n",
    "    )\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "000446da-b57a-432f-bb0d-c0f5cc714349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_input():\n",
    "    # バッチサイズに 2 を想定して、適当な文字列を作成\n",
    "    seqs = [\n",
    "        text_to_sequence(\"What is your favorite language?\"),\n",
    "        text_to_sequence(\"Hello world.\"),\n",
    "    ]\n",
    "    in_lens = torch.tensor([len(x) for x in seqs], dtype=torch.long)\n",
    "    max_len = max(len(x) for x in seqs)\n",
    "    seqs = torch.stack([torch.from_numpy(pad_1d(seq, max_len)) for seq in seqs])\n",
    "    \n",
    "    return seqs, in_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e1757caa-727a-4293-a01a-e1dd261bcbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_inout():\n",
    "    seqs, in_lens = get_dummy_input()\n",
    "   \n",
    "    # デコーダの出力（メルスペクトログラム）の教師データ\n",
    "    decoder_targets = [torch.ones(120, 80).to(\"cuda\"), torch.ones(120, 80).to(\"cuda\")]\n",
    "    \n",
    "    # stop token の教師データ\n",
    "    # stop token の予測値は確率ですが、教師データは 二値のラベルです\n",
    "    # 1 は、デコーダの出力が完了したことを表します\n",
    "    stop_tokens = torch.zeros(2, 120)\n",
    "    stop_tokens[:, -1:] = 1.0\n",
    "    \n",
    "    return seqs, in_lens, decoder_targets, stop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f510d858-f466-42c4-b64a-324939df9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語彙の定義\n",
    "characters = \"abcdefghijklmnopqrstuvwxyz!'(),-.:;? \"\n",
    "# その他特殊記号\n",
    "extra_symbols = [\n",
    "    \"^\",  # 文の先頭を表す特殊記号 <SOS>\n",
    "    \"$\",  # 文の末尾を表す特殊記号 <EOS>\n",
    "]\n",
    "_pad = \"~\"\n",
    "\n",
    "# NOTE: パディングを 0 番目に配置\n",
    "symbols = [_pad] + extra_symbols + list(characters)\n",
    "\n",
    "# 文字列⇔数値の相互変換のための辞書\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd259093-4ad8-4fa9-b7b1-971ac7671b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text):\n",
    "    # 簡易のため、大文字と小文字を区別せず、全ての大文字を小文字に変換\n",
    "    text = text.lower()\n",
    "\n",
    "    # <SOS>\n",
    "    seq = [_symbol_to_id[\"^\"]]\n",
    "\n",
    "    # 本文\n",
    "    seq += [_symbol_to_id[s] for s in text]\n",
    "\n",
    "    # <EOS>\n",
    "    seq.append(_symbol_to_id[\"$\"])\n",
    "\n",
    "    return seq\n",
    "\n",
    "\n",
    "def sequence_to_text(seq):\n",
    "    return [_id_to_symbol[s] for s in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e0b55db6-d695-4872-a146-0a11af49027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pad_mask(lengths, maxlen=None):\n",
    "    \"\"\"Make mask for padding frames\n",
    "    Args:\n",
    "        lengths (list): list of lengths\n",
    "        maxlen (int, optional): maximum length. If None, use max value of lengths.\n",
    "    Returns:\n",
    "        torch.ByteTensor: mask\n",
    "    \"\"\"\n",
    "    if not isinstance(lengths, list):\n",
    "        lengths = lengths.tolist()\n",
    "    bs = int(len(lengths))\n",
    "    if maxlen is None:\n",
    "        maxlen = int(max(lengths))\n",
    "\n",
    "    seq_range = torch.arange(0, maxlen, dtype=torch.int64)\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)\n",
    "    seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)\n",
    "    mask = seq_range_expand >= seq_length_expand\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195cd4f-e8c6-450f-97c1-883bf3666bbc",
   "metadata": {},
   "source": [
    "### トイモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "86ac0a0b-d4b8-438d-9904-f374b59307a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 33, 512])\n",
      "入力のサイズ: (2, 33)\n",
      "エンコーダの出力のサイズ: (2, 33, 512)\n",
      "デコーダの出力のサイズ: (2, 120, 80)\n",
      "Stop token のサイズ: (2, 120)\n",
      "アテンション重みのサイズ: (2, 120, 33)\n"
     ]
    }
   ],
   "source": [
    "seqs, in_lens, decoder_targets, stop_tokens = get_dummy_inout()\n",
    "model = GST_Tacotron()\n",
    "model.cuda()\n",
    "outs, outs_fine, encoder_outs, logits, att_ws, gst_att_ws = model(seqs, in_lens, decoder_targets)\n",
    "\n",
    "print(\"入力のサイズ:\", tuple(seqs.shape))\n",
    "print(\"エンコーダの出力のサイズ:\", tuple(encoder_outs.shape))\n",
    "print(\"デコーダの出力のサイズ:\", tuple(outs.shape))\n",
    "print(\"Stop token のサイズ:\", tuple(logits.shape))\n",
    "print(\"アテンション重みのサイズ:\", tuple(att_ws.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4432a-955d-4b99-919e-0541666eeae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
