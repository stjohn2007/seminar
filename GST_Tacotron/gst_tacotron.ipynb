{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8c1dca-e73d-4b08-8a7b-068eee69941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0aa683-3bda-43ee-8bd9-ac1e812786cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferenceEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        seq_len = 16000,\n",
    "        n_mel = 80,\n",
    "        conv_channels1 = 32,\n",
    "        conv_channels2 = 32,\n",
    "        conv_channels3 = 64,\n",
    "        conv_channels4 = 64,\n",
    "        conv_channels5 = 128,\n",
    "        conv_channels6 = 128,\n",
    "        n_unit = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        conv_channels_list = [\n",
    "            conv_channels1,\n",
    "            conv_channels2,\n",
    "            conv_channels3,\n",
    "            conv_channels4,\n",
    "            conv_channels5,\n",
    "            conv_channels6,\n",
    "        ]\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_size = batch_size\n",
    "        for layer in range(6):\n",
    "            seq_len = (seq_len + 1) // 2\n",
    "            n_mel = (n_mel + 1) // 2\n",
    "            \n",
    "            in_channels = 1 if layer == 0 else conv_channels_list[layer-1]\n",
    "            out_channels = conv_channels_list[layer]\n",
    "            self.convs += [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        self.convs = nn.Sequential(*self.convs)\n",
    "        \n",
    "        n_mel *= conv_channels_list[-1]\n",
    "        self.gru = nn.GRU(n_mel, n_unit, batch_first=True)\n",
    "        \n",
    "    def forward(self, seqs):\n",
    "        out = self.convs(seqs) # (B, 1, seq_len, n_mel) -> (B, channels, new_seq_len, new_n_mel)\n",
    "        out = torch.squeeze(out.reshape(self.batch_size, 1, out.shape[2], -1)) # (B, channels, new_seq_len, new_n_mel) -> (B, new_seq_len, channels * new_n_mel)\n",
    "        out = self.gru(out) # (B, new_seq_len, channels * new_n_mel) -> (B, new_seq_len, n_unit)\n",
    "        return out[0], out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb1be25-89f0-406e-85f8-1386f96f1529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 250, 128])\n"
     ]
    }
   ],
   "source": [
    "ref = ReferenceEncoder(10)\n",
    "input = torch.zeros(10, 1, 16000, 80)\n",
    "ref_output, hidden_state = ref(input)\n",
    "print(ref_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f46e2f3-f18d-4b3c-8a08-f9744f2da06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qiita.com/m__k/items/646044788c5f94eadc8d\n",
    "\n",
    "class StyleTokenLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        emb_size = 256,\n",
    "        n_tokens = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.emb_size = emb_size,\n",
    "        self.softmax = nn.Softmax(dim=2) # n_token方向にsoftmaxを取りたい\n",
    "        self.tokens = torch.randn(n_tokens, emb_size)\n",
    "    \n",
    "    def forward(self, query):\n",
    "        query = torch.cat((query, query), 2) # この実装があってるかどうかわからない, ReferenceEncoderの出力次元が128でtokenのembeddingの次元が256なのでqueryを2つ重ねている\n",
    "        tokens = self.tanh(self.tokens)\n",
    "        key = tokens.expand(self.batch_size, *tokens.shape) # (n_token, emb_size) -> (B, n_token, emb_size)\n",
    "        value = key\n",
    "        \n",
    "        key = torch.transpose(key, 1, 2) # (B, n_token, emb_size) -> (B, emb_size, n_token)\n",
    "        s = torch.bmm(query, key) / self.emb_size[0]**(1/2) # (B, seq_len, emb_size) @ (B, emb_size, n_token) -> (B, seq_len, n_token)  論文ではcosine similarityを用いていたが実装の簡単のためdot-productにした\n",
    "        attention_weight = self.softmax(s) # (B, seq_len, n_token) -> (B, seq_len, n_token)\n",
    "        out = torch.bmm(attention_weight, value) #(B, seq_len, n_token) @ (B, n_token, emb_size) -> (B, seq_len, emb_size)\n",
    "        \n",
    "        return out, attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbe7f05-623b-431d-876c-fa3137802e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 250, 256])\n"
     ]
    }
   ],
   "source": [
    "style = StyleTokenLayer(10)\n",
    "style_output, attention_weight = style(ref_output)\n",
    "print(style_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bad4ed-b428-42b8-969b-7272aec7c3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
